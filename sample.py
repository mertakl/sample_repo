
"""
SSE streaming endpoint for LLM Assistant Backend.

Facade connects via SSE client, forwards to WebSocket client.
Flow: Validate → Reconstruct context → Call LLM → Stream deltas → Store response
"""

import asyncio
import json
import logging
from typing import Any, AsyncIterator, Literal
from uuid import UUID

from django.http import StreamingHttpResponse
from django.views.decorators.csrf import csrf_protect
from django.views.decorators.http import require_http_methods
from pydantic import BaseModel, Field, ValidationError
from rest_framework import status
from rest_framework.decorators import api_view

from llm_assistant.apps.core.commands import StreamChatMessageCommand
from llm_assistant.apps.core.contracts.messages import StreamErrorMessage
from llm_assistant.apps.core.services.chat_streaming_service import ChatStreamingService

logger = logging.getLogger(__name__)


# ============================================================================
# Request Models
# ============================================================================


class StreamChatRequest(BaseModel):
    """SSE streaming request from Facade."""

    action: Literal["chat.start", "chat.message"] = Field(
        ..., description="Chat action type"
    )
    discussion_id: UUID = Field(..., description="UUID generated by Facade")
    content: str = Field(
        ..., min_length=1, max_length=10000, description="User's message"
    )
    initiator_id: str = Field(default="anonymous", description="User identifier")
    business_context_id: str = Field(
        ..., description="Business context (same as discussion_id)"
    )


# ============================================================================
# SSE Formatting
# ============================================================================


def format_sse_event(event_type: str, data: dict) -> str:
    """
    Format data as SSE event.
    
    Args:
        event_type: SSE event type
        data: Event payload
        
    Returns:
        SSE-formatted string
    """
    return f"event: {event_type}\ndata: {json.dumps(data)}\n\n"


def create_error_event(
    error_code: str,
    error_text: str,
    discussion_id: UUID | None = None,
    message_id: UUID | None = None,
    request_message_id: UUID | None = None,
) -> str:
    """
    Create SSE error event.
    
    Args:
        error_code: Error code identifier
        error_text: Human-readable error message
        discussion_id: Optional discussion UUID
        message_id: Optional message UUID
        request_message_id: Optional request message UUID
        
    Returns:
        SSE-formatted error event
    """
    error_msg = StreamErrorMessage.model_construct(
        action="stream.error",
        payload={
            "message_id": message_id,
            "discussion_id": discussion_id,
            "request_message_id": request_message_id,
            "error_code": error_code,
            "error_text": error_text,
        },
    )
    return format_sse_event("stream.error", error_msg.model_dump())


# ============================================================================
# SSE Event Generation
# ============================================================================


async def sse_event_generator(
    chat_service: ChatStreamingService,
    command: StreamChatMessageCommand,
) -> AsyncIterator[str]:
    """
    Generate SSE events from ChatStreamingService.
    
    Service yields StreamMessage objects - serialize to SSE wire format.
    ChatStreamingService orchestrates LLM context reconstruction and streaming.
    
    Args:
        chat_service: Service handling chat streaming logic
        command: Command containing chat parameters
        
    Yields:
        SSE-formatted event strings
    """
    logger.info(
        "SSE generator starting - discussion_id=%s, action=%s",
        command.discussion_id,
        command.action,
    )
    event_count = 0

    try:
        async for message in chat_service.stream_chat_message(command):
            event_count += 1

            # Extract message data and action type
            message_dict = (
                message.model_dump()
                if hasattr(message, "model_dump")
                else message
            )
            action = self._extract_action(message_dict, message)

            logger.debug(
                "SSE event #%d - action=%s, discussion_id=%s",
                event_count,
                action,
                command.discussion_id,
            )

            # Yield SSE-formatted event
            yield format_sse_event(action, message_dict)

            # Small delay to ensure proper flushing
            await asyncio.sleep(0.001)

        logger.info(
            "SSE generator completed - events=%d, discussion_id=%s",
            event_count,
            command.discussion_id,
        )

    except Exception as e:
        logger.exception(
            "SSE generation failed - discussion_id=%s, error=%s",
            command.discussion_id,
            str(e),
        )
        yield create_error_event(
            error_code="stream_generation_error",
            error_text=str(e),
            discussion_id=command.discussion_id,
        )


def _extract_action(message_dict: Any, message: Any) -> str:
    """Extract action from message dict or object."""
    if isinstance(message_dict, dict):
        return message_dict.get("action", "unknown")
    return getattr(message, "action", "unknown")


# ============================================================================
# Request Handling
# ============================================================================


def validate_and_parse_request(request_data: dict) -> StreamChatRequest:
    """
    Validate and parse incoming request data.
    
    Args:
        request_data: Raw request data
        
    Returns:
        Validated StreamChatRequest
        
    Raises:
        ValidationError: If validation fails
    """
    return StreamChatRequest(**request_data)


def create_command(request_data: StreamChatRequest) -> StreamChatMessageCommand:
    """
    Create command from validated request.
    
    Args:
        request_data: Validated request data
        
    Returns:
        StreamChatMessageCommand for processing
    """
    return StreamChatMessageCommand(
        initiator_id=request_data.initiator_id,
        business_context_id=request_data.business_context_id,
        action=request_data.action,
        discussion_id=request_data.discussion_id,
        content=request_data.content,
    )


def create_sse_response(
    generator: AsyncIterator[str],
    status_code: int = status.HTTP_200_OK,
) -> StreamingHttpResponse:
    """
    Create SSE streaming response with proper headers.
    
    Args:
        generator: Async generator yielding SSE events
        status_code: HTTP status code
        
    Returns:
        StreamingHttpResponse configured for SSE
    """
    response = StreamingHttpResponse(
        generator,
        content_type="text/event-stream",
        status=status_code,
    )
    response["Cache-Control"] = "no-cache"
    response["X-Accel-Buffering"] = "no"  # Disable nginx buffering
    return response


def create_error_response(
    error_code: str,
    error_text: str,
    status_code: int,
    discussion_id: UUID | None = None,
) -> StreamingHttpResponse:
    """
    Create error response in SSE format.
    
    Args:
        error_code: Error code identifier
        error_text: Error message
        status_code: HTTP status code
        discussion_id: Optional discussion UUID
        
    Returns:
        StreamingHttpResponse with error event
    """
    error_event = create_error_event(
        error_code=error_code,
        error_text=error_text,
        discussion_id=discussion_id,
    )
    return create_sse_response(
        iter([error_event]),
        status_code=status_code,
    )


# ============================================================================
# View
# ============================================================================


@api_view(["POST"])
@require_http_methods(["POST"])
@csrf_protect
def stream_chat_sse(request: Any) -> StreamingHttpResponse:
    """
    POST /api/v1/chat/stream - SSE streaming endpoint.
    
    Accepts JSON from Facade, validates request, reconstructs LLM context,
    and streams responses back as Server-Sent Events.
    
    Args:
        request: Django request object with chat data
        
    Returns:
        StreamingHttpResponse with SSE events
    """
    logger.info(
        "SSE request received - method=%s, path=%s",
        request.method,
        request.path,
    )

    try:
        # Parse and validate request
        logger.debug("Parsing request data")
        request_data = validate_and_parse_request(request.data)

        logger.info(
            "Request validated - discussion_id=%s, action=%s, content_len=%d",
            request_data.discussion_id,
            request_data.action,
            len(request_data.content),
        )

        # Create command and initialize services
        command = create_command(request_data)
        chat_service = ChatStreamingService()

        logger.info(
            "Services initialized - starting stream for discussion_id=%s",
            command.discussion_id,
        )

        # Return SSE stream response
        return create_sse_response(
            sse_event_generator(chat_service, command)
        )

    except ValidationError as e:
        logger.warning("Validation failed - error=%s", str(e))
        return create_error_response(
            error_code="validation_error",
            error_text=str(e),
            status_code=status.HTTP_400_BAD_REQUEST,
        )

    except Exception as e:
        logger.exception("Unexpected error - error=%s", str(e))
        return create_error_response(
            error_code="internal_error",
            error_text=str(e),
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        )
